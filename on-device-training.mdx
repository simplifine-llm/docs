---
title: 'On-Device Training Quickstart'
description: "Learn how to train models on your device using Simplifine."
icon: "computer"
---

## On-device training

This section shows how to use Simplifine to train an LLM for supervised fine-tuning on-device. 

## Config objects for simplifine

There are different types of dataclasses (or configs) used in simplifine. We configure the prompt config (sft prompt config in this case) and huggingface's SFTConfig in this example. We also define the name of the model and the dataset that is being used.

```python
from simplifine_alpha import train_engine
from trl import SFTConfig

dataset_name = 'community-datasets/fake_news_english'
model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'

# defining prompt config
sft_prompt_config = train_engine.sftPromptConfig(
  keys = ['url_of_article', 'fake_or_satire'],
  template = "###URL: {url_of_article}. \n###CLS: {fake_or_satire}",
  response_template = '. \n###CLS: ',
  use_chat_template=True
  )

# defining training config
sft_config = SFTConfig(
    output_dir='/content/fake_news_english_phi3',
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=1e-5,
    num_train_epochs=2,
    report_to='none',
    fp16=True,
    gradient_checkpointing=True,
)
```


### Using a Local Dataset
If you don't want to use a Hugging Face dataset, set `data_from_hf` to False and provide a local dataset. The example below shows how to manually create a dataset:

``` python
data = {'url_of_article': ['url1','url2','url3'],
        'fake_or_satire':[1,0,1]
      }
```

## Initiating local training
You can finally call the function to initate the training like this: 

```python
train_engine.sft_train(model_name=model_name, dataset_name=dataset_name,
                       sft_config = sft_config, sft_prompt_config=sft_prompt_config,
                       use_zero=False, use_ddp=False
                      )
```

Note that this has DDP and ZeRO disabled. If you wish to use these features, then this function needs to be in a script, and you need to run the script using `torchrun`.
