---
title: 'Quickstart'
description: 'Start finetuning your language models with Simplifine in under 5 mins!'
---

## Finetuning a Model to be a Multi-Label Classifier

Let's start by training a multi-label classifier using the `hf_clf_multi_label_train` function.

```python
from simplifine import hf_clf_multi_label_train

model_name = 'openai-community/gpt2'
inputs = ['I love this place', 'I hate this place', 'I am neutral about this place']
labels = [[1, 0, 0], [0, 1, 0], [1, 0, 1]]
num_labels = 3

hf_clf_multi_label_train(model_name, inputs=inputs, labels=labels, num_labels=num_labels)

```


## Fine-tuning an Embedder with Contrastive Learning
You can also fine-tune an embedder with contrastive learning using the `hf_finetune_embedder_contrastive` function.
  
  ```python
  from simplifine import hf_finetune_embedder_contrastive

queries = [
    'q1: How would you rate your experience with our customer service?',
    'q2: Are you satisfied with the quality of our product?',
    'q3: Would you recommend our services to others?',
    # Add more queries as needed
]

positives = [
    'Excellent, very satisfied.',
    'Yes, absolutely!',
    'Definitely, without a doubt.',
    # Add more positive responses as needed
]

negatives = [
    'Poor, very dissatisfied.',
    'No, not at all.',
    'No, I wouldnâ€™t.',
    # Add more negative responses as needed
]

hf_finetune_embedder_contrastive(
    model_name='sentence-transformers/paraphrase-MiniLM-L6-v2',
    from_hf=False,
    queries=queries,
    positives=positives,
    negatives=negatives,
    use_matryoshka=True,
    matryoshka_dimensions=[128, 256]
)
  ```


## Fine-tuning a Question Answering (QA) Model
Finally, let's fine-tune a QA model using the `hf_finetune_llm_qa` function.

```python
from your_library_name import hf_finetune_llm_qa
from datasets import load_dataset

ds = load_dataset("ali77sina/SEC-QA-sorted-chunks")
questions = ds['train']['questions'][:100]
contexts = ds['train']['sorted_chunks'][:100]
answers = ds['train']['answers'][:100]

model_name = 'bigscience/bloom-560m'

hf_finetune_llm_qa(
    model_name,
    from_hf=False,
    queries=questions,
    context=contexts,
    answers=answers
)
```

## Fine-tuning a Model with a Custom Template

Next, we'll fine-tune a model using the `hf_sft` function with a custom template.

```python
from simplifine import hf_sft

model_name = 'EleutherAI/pythia-6.9b-deduped'
keys = ['questions', 'sorted_chunks', 'answers']

template = '''### Question: {questions}
### Context: {sorted_chunks}
### Answer: {answers}'''

hf_sft(model_name, keys=keys, template=template)

```


<CardGroup>

<Card title="License" icon="file" href="/license/license">
  We are Apache 2.0 licensed. Check out our license for more details.
</Card>

<Card
  title="Contribute to our Community"
  icon="github"
  href="https://github.com/simplifine-llm/Simplifine"
>
  Join our community and contribute to our open-source project to make LLM fine-tuning accessible to everyone.
</Card>


</CardGroup>
